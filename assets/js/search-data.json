{
  
    
        "post0": {
            "title": "Fairness",
            "content": "import numpy as np import pandas as pd import matplotlib.pylab as plt from sklearn.datasets import load_boston from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklego.preprocessing import InformationFilter . X, y = load_boston(return_X_y=True) pipe = Pipeline([ (&quot;scale&quot;, StandardScaler()), (&quot;model&quot;, LinearRegression()) ]) plt.scatter(pipe.fit(X, y).predict(X), y) plt.xlabel(&quot;predictions&quot;) plt.ylabel(&quot;actual&quot;) plt.title(&quot;plot that suggests it&#39;s not bad&quot;); . We could stop our research here if we think that our MSE is &quot;good enough&quot; but this would be dangerous. To find out why, we should look at the variables that are being used in our model. . print(load_boston()[&#39;DESCR&#39;][:1200]) . .. _boston_dataset: Boston house prices dataset **Data Set Characteristics:** :Number of Instances: 506 :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. :Attribute Information (in order): - CRIM per capita crime rate by town - ZN proportion of residential land zoned for lots over 25,000 sq.ft. - INDUS proportion of non-retail business acres per town - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) - NOX nitric oxides concentration (parts per 10 million) - RM average number of rooms per dwelling - AGE proportion of owner-occupied units built prior to 1940 - DIS weighted distances to five Boston employment centres - RAD index of accessibility to radial highways - TAX full-value property-tax rate per $10,000 - PTRATIO pupil-teacher ratio by town - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town - LSTAT % lower status of the population - MEDV Median value of owner . This dataset contains features like &quot;lower status of population&quot; and &quot;the proportion of blacks by town&quot;. This is bad. There&#39;s a real possibility that our model will overfit on MSE and underfit on fairness when we want to apply it. Scikit-Lego has some support to deal with fairness issues like this one. . Dealing with issues such as fairness in machine learning can in general be done in three ways: . Data preprocessing | Model contraints | Prediction postprocessing. | . But before we can dive into methods for getting more fair predictions, we first need to define how to measure fairness . Measuring fairness for Regression . Measuring fairness can be done in many ways but we&#39;ll consider one definition: the output of the model is fair with regards to groups $A$ and $B$ if prediction has a distribution independant if group $A$ or $B$. In laymans terms: if group $A$ and $B$ don&#39;t get the same predictions: no bueno. . Formally, this can be written as: . $$fairness = left lvert frac{1}{|Z_1|} sum_{i in Z_1} hat{y}_{i} - frac{1}{|Z_0|} sum_{i in Z_0} hat{y}_{i} right rvert$$ . where $Z_1$ is the subset of the population where our sensitive attribute is true, and $Z_0$ the subset of the population where the sensitive attribute is false . To estimate this we&#39;ll use bootstrap sampling to measure the models bias. . Measuring fairness for Classification . A common method for measuring fairness is demographic parity1, for example through the p-percent metric. The idea behind it is that it requires that a decision — such as accepting or denying a loan application — be independent of the protected attribute. In other words, we expect the positive rate in both groups to be the same. In the case of a binary decision $ hat{y}$ and a binary protected attribute $z$, this constraint can be formalized by asking that . $$P( hat{y}=1 | z=0)=P( hat{y}=1 | z=1)$$ . You can turn this into a metric by calculating how far off this exact equality your decision process is. This metric is called the p% score . $$ text{p% score} = min left( frac{P( hat{y}=1 | z=1)}{P( hat{y}=1 | z=0)}, frac{P( hat{y}=1 | z=0)}{P( hat{y}=1 | z=1)} right)$$ . In other words, membership in a protected class should have no correlation with the decision. . In sklego this metric is implemented in sklego.metrics.p_percent_score and it works as follows: . from sklego.metrics import p_percent_score from sklearn.linear_model import LogisticRegression sensitive_classification_dataset = pd.DataFrame({ &quot;x1&quot;: [1, 0, 1, 0, 1, 0, 1, 1], &quot;x2&quot;: [0, 0, 0, 0, 0, 1, 1, 1], &quot;y&quot;: [1, 1, 1, 0, 1, 0, 0, 0]} ) X, y = sensitive_classification_dataset.drop(columns=&#39;y&#39;), sensitive_classification_dataset[&#39;y&#39;] mod_unfair = LogisticRegression(solver=&#39;lbfgs&#39;).fit(X, y) print(&#39;p_percent_score:&#39;, p_percent_score(sensitive_column=&quot;x2&quot;)(mod_unfair, X)) . p_percent_score: 0 . /Users/vincent/Development/scikit-lego/sklego/metrics.py:77: RuntimeWarning: No samples with y_hat == 1 for x2 == 1, returning 0 RuntimeWarning, . Of course, no metric is perfect. If we for example use this in a loan approval situation, the demographic parity only looks at loans given, and not at the rate at which loans are paid back. That might result in a lower percentage of qualified people who are given loans in one population than in another. Another way of measuring fairness could be therefore to measure equal opportunity2 instead. This constraint would boil down to: . $$P( hat{y}=1 | z=0, y=1)=P( hat{y}=1 | z=1, y=1)$$ . and be turned into a metric in the same way as above: . $$ text{equality of opportunity} = min left( frac{P( hat{y}=1 | z=1, y=1)}{P( hat{y}=1 | z=0, y=1)}, frac{P( hat{y}=1 | z=0, y=1)}{P( hat{y}=1 | z=1, y=1)} right)$$ . We can see in the example below that the equal opportunity score does not differ for the models as long as the records where y_true = 1 are predicted correctly. . from sklego.metrics import equal_opportunity_score from sklearn.linear_model import LogisticRegression import types sensitive_classification_dataset = pd.DataFrame({ &quot;x1&quot;: [1, 0, 1, 0, 1, 0, 1, 1], &quot;x2&quot;: [0, 0, 0, 0, 0, 1, 1, 1], &quot;y&quot;: [1, 1, 1, 0, 1, 0, 0, 1]} ) X, y = sensitive_classification_dataset.drop(columns=&#39;y&#39;), sensitive_classification_dataset[&#39;y&#39;] mod_1 = types.SimpleNamespace() mod_1.predict = lambda X: np.array([1, 0, 1, 0, 1, 0, 1, 1]) print(&#39;equal_opportunity_score:&#39;, equal_opportunity_score(sensitive_column=&quot;x2&quot;)(mod_1, X, y)) mod_1.predict = lambda X: np.array([1, 0, 1, 0, 1, 0, 0, 1]) print(&#39;equal_opportunity_score:&#39;, equal_opportunity_score(sensitive_column=&quot;x2&quot;)(mod_1, X, y)) mod_1.predict = lambda X: np.array([1, 0, 1, 0, 1, 0, 0, 0]) print(&#39;equal_opportunity_score:&#39;, equal_opportunity_score(sensitive_column=&quot;x2&quot;)(mod_1, X, y)) . equal_opportunity_score: 0.75 equal_opportunity_score: 0.75 equal_opportunity_score: 0.0 . /Users/vincent/Development/scikit-lego/sklego/metrics.py:151: RuntimeWarning: divide by zero encountered in double_scalars score = np.minimum(p_y1_z1 / p_y1_z0, p_y1_z0 / p_y1_z1) . ## Data preprocessing When doing data preprocessing we&#39;re trying to remove any bias caused by the sensitive variable from the input dataset. By doing this, we remain flexible in our choice of models. ### Information Filter This is a great opportunity to use the `InformationFilter` which can filter the information of these two sensitive columns away as a transformation step. It does this by projecting all vectors away such that the remaining dataset is orthogonal to the sensitive columns. #### How it Works The `InformationFilter` uses a variant of the [gram smidt process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process) to filter information out of the dataset. We can make it visual in two dimensions; ![](images/projections.png) To explain what occurs in higher dimensions we need to resort to maths. Take a training matrix $X$ that contains columns $x_1, ..., x_k$. If we assume columns $x_1$ and $x_2$ to be the sensitive columns then the information filter will filter out information using this approach; $$ begin{split} v_1 &amp; = x_1 v_2 &amp; = x_2 - frac{x_2 v_1}{v_1 v_1} v_3 &amp; = x_3 - frac{x_3 v_1}{v_1 v_1} - frac{x_3 v_2}{v_2 v_2} ... v_k &amp; = x_k - frac{x_k v_1}{v_1 v_1} - frac{x_k&#39; v_2}{v_2 v_2} end{split} $$ Concatenating our vectors (but removing the sensitive ones) gives us a new training matrix $X_{ text{more fair}} = [v_3, ..., v_k]$. #### Experiment We will demonstrate the effect of applying this by benchmarking three things: 1. Keep $X$ as is. 2. Drop the two columns that are sensitive. 3. Use the information filter We&#39;ll use the regression metric defined above to show the differences in fairness . X, y = load_boston(return_X_y=True) df = pd.DataFrame(X, columns=[&#39;crim&#39;,&#39;zn&#39;,&#39;indus&#39;,&#39;chas&#39;,&#39;nox&#39;, &#39;rm&#39;,&#39;age&#39;,&#39;dis&#39;,&#39;rad&#39;,&#39;tax&#39;,&#39;ptratio&#39;, &#39;b&#39;,&#39;lstat&#39;]) X_drop = df.drop(columns=[&quot;lstat&quot;, &quot;b&quot;]) X_fair = InformationFilter([&quot;lstat&quot;, &quot;b&quot;]).fit_transform(df) X_fair = pd.DataFrame(X_fair, columns=[n for n in df.columns if n not in [&#39;b&#39;, &#39;lstat&#39;]]) . def simple_mod(): return Pipeline([(&quot;scale&quot;, StandardScaler()), (&quot;mod&quot;, LinearRegression())]) base_mod = simple_mod().fit(X, y) drop_mod = simple_mod().fit(X_drop, y) fair_mod = simple_mod().fit(X_fair, y) base_pred = base_mod.predict(X) drop_pred = drop_mod.predict(X_drop) fair_pred = fair_mod.predict(X_fair) . We can see that the coefficients of the three models are indeed different. . pd.DataFrame([base_mod.steps[1][1].coef_, drop_mod.steps[1][1].coef_, fair_mod.steps[1][1].coef_], columns=df.columns) . crim zn indus chas nox rm age dis rad tax ptratio b lstat . 0 -0.928146 | 1.081569 | 0.140900 | 0.681740 | -2.056718 | 2.674230 | 0.019466 | -3.104044 | 2.662218 | -2.076782 | -2.060607 | 0.849268 | -3.743627 | . 1 -1.581396 | 0.911004 | -0.290074 | 0.884936 | -2.567870 | 4.264702 | -1.270735 | -3.331836 | 2.215737 | -2.056246 | -2.154600 | NaN | NaN | . 2 -0.763568 | 1.028051 | 0.061393 | 0.697504 | -1.605464 | 6.846774 | -0.057920 | -2.537602 | 1.935058 | -1.779825 | -2.793069 | NaN | NaN | . # we&#39;re using lstat to select the group to keep things simple selector = df[&quot;lstat&quot;] &gt; np.quantile(df[&quot;lstat&quot;], 0.5) def bootstrap_means(preds, selector, n=2500, k=25): grp1 = np.random.choice(preds[selector], (n, k)).mean(axis=1) grp2 = np.random.choice(preds[~selector], (n, k)).mean(axis=1) return grp1 - grp2 . 1. Original Situation . plt.figure(figsize=(10, 5)) plt.subplot(121) plt.scatter(base_pred, y) plt.title(f&quot;MSE: {mean_squared_error(y, base_pred)}&quot;) plt.subplot(122) plt.hist(bootstrap_means(base_pred, selector), bins=30, density=True, alpha=0.8) plt.title(f&quot;Fairness Proxy&quot;); . 2. Drop two columns . plt.figure(figsize=(10, 5)) plt.subplot(121) plt.scatter(drop_pred, y) plt.title(f&quot;MSE: {mean_squared_error(y, drop_pred)}&quot;) plt.subplot(122) plt.hist(bootstrap_means(base_pred, selector), bins=30, density=True, alpha=0.8) plt.hist(bootstrap_means(drop_pred, selector), bins=30, density=True, alpha=0.8) plt.title(f&quot;Fairness Proxy&quot;); . 3. Use the Information Filter . plt.figure(figsize=(10, 5)) plt.subplot(121) plt.scatter(fair_pred, y) plt.title(f&quot;MSE: {mean_squared_error(y, fair_pred)}&quot;) plt.subplot(122) plt.hist(bootstrap_means(base_pred, selector), bins=30, density=True, alpha=0.8) plt.hist(bootstrap_means(fair_pred, selector), bins=30, density=True, alpha=0.8) plt.title(f&quot;Fairness Proxy&quot;); . There definitely is a balance between fairness and model accuracy. Which model you&#39;ll use depends on the world you want to create by applying your model. . Note that you can combine models here to make an ensemble too. You can also use the difference between the 1st and last model as a proxy for bias. . Model constraints . Another way we could tackle this fairness problem would be to explicitly take fairness into account when optimizing the parameters of our model. This is implemented in the DemographicParityClassifier as well as the EqualOpportunityClassifier. . Both these models are built as an extension of basic logistic regression. Where logistic regression optimizes the following problem: . $$ begin{array}{cl} { operatorname{minimize}} &amp; - sum_{i=1}^{N} log p left(y_{i} | mathbf{x}_{i}, boldsymbol{ theta} right) end{array} $$We would like to instead optimize this: . $$ begin{array}{cl}{ operatorname{minimize}} &amp; - sum_{i=1}^{N} log p left(y_{i} | mathbf{x}_{i}, boldsymbol{ theta} right) { text { subject to }} &amp; text{fairness} geq mathbf{c} end{array} $$ . Demographic Parity Classifier . The p% score discussed above is a nice metric but unfortunately it is rather hard to directly implement in the formulation into our model as it is a non-convex function making it difficult to optimize directly. Also, as the p% rule only depends on which side of the decision boundary an observation lies, it is invariant in small changes in the decision boundary. This causes large saddle points in the objective making optimization even more difficult . Instead of optimizing for the p% directly, we approximate it by taking the covariance between the users’ sensitive attributes, $z$m, and the decision boundary. This results in the following formulation of our DemographicParityClassifier. . $$ begin{array}{cl}{ operatorname{minimize}} &amp; - sum_{i=1}^{N} log p left(y_{i} | mathbf{x}_{i}, boldsymbol{ theta} right) { text { subject to }} &amp; { frac{1}{N} sum_{i=1}^{N} left( mathbf{z}_{i}- overline{ mathbf{z}} right) d_ boldsymbol{ theta} left( mathbf{x}_{i} right) leq mathbf{c}} {} &amp; { frac{1}{N} sum_{i=1}^{N} left( mathbf{z}_{i}- overline{ mathbf{z}} right) d_{ boldsymbol{ theta}} left( mathbf{x}_{i} right) geq- mathbf{c}} end{array} $$ . Let&#39;s see what the effect of this is. As this is a Classifier and not a Regressor, we transform the target to a binary variable indicating whether it is above or below the median. Our p% metric also assumes a binary indicator for sensitive columns so we do the same for our lstat column. . Fitting the model is as easy as fitting a normal sklearn model. We just need to supply the columns that should be treated as sensitive to the model, as well as the maximum covariance we want to have. . from sklego.linear_model import DemographicParityClassifier from sklearn.linear_model import LogisticRegression from sklego.metrics import p_percent_score from sklearn.metrics import accuracy_score, make_scorer from sklearn.model_selection import GridSearchCV df_clf = df.assign(lstat=lambda d: d[&#39;lstat&#39;] &gt; np.median(d[&#39;lstat&#39;])) y_clf = y &gt; np.median(y) normal_classifier = LogisticRegression(solver=&#39;lbfgs&#39;) normal_classifier.fit(df_clf, y_clf) fair_classifier = DemographicParityClassifier(sensitive_cols=&quot;lstat&quot;, covariance_threshold=0.5) fair_classifier.fit(df_clf, y_clf); . /Users/vincent/Development/scikit-lego/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations. &#34;of iterations.&#34;, ConvergenceWarning) . Comparing the two models on their p% scores also shows that the fair classifier has a much higher fairness score at a slight cost in accuracy. . We&#39;ll compare these two models by doing a gridsearch on the effect of the covariance_threshold. . import warnings fair_classifier = GridSearchCV(estimator=DemographicParityClassifier(sensitive_cols=&quot;lstat&quot;, covariance_threshold=0.5), param_grid={&quot;estimator__covariance_threshold&quot;: np.linspace(0.01, 1.00, 20)}, cv=5, refit=&quot;accuracy_score&quot;, return_train_score=True, scoring={&quot;p_percent_score&quot;: p_percent_score(&#39;lstat&#39;), &quot;accuracy_score&quot;: make_scorer(accuracy_score)}) with warnings.catch_warnings(): warnings.simplefilter(&quot;ignore&quot;) fair_classifier.fit(df_clf, y_clf); pltr = (pd.DataFrame(fair_classifier.cv_results_) .set_index(&quot;param_estimator__covariance_threshold&quot;)) p_score = p_percent_score(&#39;lstat&#39;)(normal_classifier, df_clf, y_clf) acc_score = accuracy_score(normal_classifier.predict(df_clf), y_clf) . The results of the grid search are shown below. Note that the logistic regression results are of the train set, not the test set. We can see that the increase in fairness comes at the cost of accuracy but this might literally be a fair tradeoff. . plt.figure(figsize=(12, 3)) plt.subplot(121) plt.plot(np.array(pltr.index), pltr[&#39;mean_test_p_percent_score&#39;], label=&#39;fairclassifier&#39;) plt.plot(np.linspace(0, 1, 2), [p_score for _ in range(2)], label=&#39;logistic-regression&#39;) plt.xlabel(&quot;covariance threshold&quot;) plt.legend() plt.title(&quot;p% score&quot;) plt.subplot(122) plt.plot(np.array(pltr.index), pltr[&#39;mean_test_accuracy_score&#39;], label=&#39;fairclassifier&#39;) plt.plot(np.linspace(0, 1, 2), [acc_score for _ in range(2)], label=&#39;logistic-regression&#39;) plt.xlabel(&quot;covariance threshold&quot;) plt.legend() plt.title(&quot;accuracy&quot;); . Equal opportunity . In the same spirit as the DemographicParityClassifier discussed above, there is also an EqualOpportunityClassifier which optimizes . $$ begin{array}{cl}{ operatorname{minimize}} &amp; - sum_{i=1}^{N} log p left(y_{i} | mathbf{x}_{i}, boldsymbol{ theta} right) { text { subject to }} &amp; { frac{1}{POS} sum_{i=1}^{POS} left( mathbf{z}_{i}- overline{ mathbf{z}} right) d boldsymbol{ theta} left( mathbf{x}_{i} right) leq mathbf{c}} {} &amp; { frac{1}{POS} sum_{i=1}^{POS} left( mathbf{z}_{i}- overline{ mathbf{z}} right) d_{ boldsymbol{ theta}} left( mathbf{x}_{i} right) geq- mathbf{c}} end{array}$$where POS is the subset of the population where y_true = positive_target . import warnings from sklego.linear_model import EqualOpportunityClassifier fair_classifier = GridSearchCV( estimator=EqualOpportunityClassifier( sensitive_cols=&quot;lstat&quot;, covariance_threshold=0.5, positive_target=True, ), param_grid={&quot;estimator__covariance_threshold&quot;: np.linspace(0.001, 1.00, 20)}, cv=5, n_jobs=-1, refit=&quot;accuracy_score&quot;, return_train_score=True, scoring={&quot;p_percent_score&quot;: p_percent_score(&#39;lstat&#39;), &quot;equal_opportunity_score&quot;: equal_opportunity_score(&#39;lstat&#39;), &quot;accuracy_score&quot;: make_scorer(accuracy_score)} ) with warnings.catch_warnings(): warnings.simplefilter(&quot;ignore&quot;) fair_classifier.fit(df_clf, y_clf); pltr = (pd.DataFrame(fair_classifier.cv_results_) .set_index(&quot;param_estimator__covariance_threshold&quot;)) p_score = p_percent_score(&#39;lstat&#39;)(normal_classifier, df_clf, y_clf) acc_score = accuracy_score(normal_classifier.predict(df_clf), y_clf) . plt.figure(figsize=(12, 3)) plt.subplot(121) plt.plot(np.array(pltr.index), pltr[&#39;mean_test_equal_opportunity_score&#39;], label=&#39;fairclassifier&#39;) plt.plot(np.linspace(0, 1, 2), [p_score for _ in range(2)], label=&#39;logistic-regression&#39;) plt.xlabel(&quot;covariance threshold&quot;) plt.legend() plt.title(&quot;equal opportunity score&quot;) plt.subplot(122) plt.plot(np.array(pltr.index), pltr[&#39;mean_test_accuracy_score&#39;], label=&#39;fairclassifier&#39;) plt.plot(np.linspace(0, 1, 2), [acc_score for _ in range(2)], label=&#39;logistic-regression&#39;) plt.xlabel(&quot;covariance threshold&quot;) plt.legend() plt.title(&quot;accuracy&quot;); . Sources . M. Zafar et al. (2017), Fairness Constraints: Mechanisms for Fair Classification | M. Hardt, E. Price and N. Srebro (2016), Equality of Opportunity in Supervised Learning |",
            "url": "https://nickmccarty.github.io/til/2020/05/03/_05_04-algorithmic-fairness.html",
            "relUrl": "/2020/05/03/_05_04-algorithmic-fairness.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "How to Create an Interactive Scatterplot with Altair",
            "content": ". Tip: Here are the required libraries and dataset. . #collapse-hide import pandas as pd import altair as alt movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; . . Example: Interactive Dropdown and Tooltips . Chart taken from this repo, specifically this notebook. . #collapse-hide df = pd.read_json(movies) # load movies data genres = df[&#39;Major_Genre&#39;].unique() # get unique field values genres = list(filter(lambda d: d is not None, genres)) # filter out null values genres.sort() # sort alphabetically mpaa = [&#39;G&#39;, &#39;PG&#39;, &#39;PG-13&#39;, &#39;R&#39;, &#39;NC-17&#39;, &#39;Not Rated&#39;] # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;], opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . .",
            "url": "https://nickmccarty.github.io/til/altair/jupyter/python/2020/05/03/_04_28_altair_test.html",
            "relUrl": "/altair/jupyter/python/2020/05/03/_04_28_altair_test.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
  
    
        ,"post3": {
            "title": "How to Visualize the Central Limit Theorem Using Python",
            "content": ". Note: The code required to replicate the visualizations below is provided in the Show Code dropdowns, but the modules thinkstats2 and thinkplot must first be pulled from here. . #collapse-hide %matplotlib inline import numpy as np import pandas as pd from scipy.stats import norm from scipy.stats import expon import random import thinkstats2 import thinkplot from matplotlib import MatplotlibDeprecationWarning from warnings import simplefilter simplefilter(&#39;ignore&#39;, MatplotlibDeprecationWarning) . . tl;dr . If you add up independent variates from a distribution with finite mean and variance, the sum converges on a normal distribution. The sum of exponential variates converges to normal as sample size increases. The lognormal distribution has higher variance, so it requires a larger sample size before it converges to normal. The Pareto distribution violates the requirements of the CLT and does not generally converge to normal. If the random variates are correlated, that also violates the CLT, so the sums don&#39;t generally converge. . Distribution of Independent Variates with Finite Mean and Variance . If you add up independent variates from a distribution with finite mean and variance, the sum converges on a normal distribution. The following plot shows how the sum of exponential variates converges to normal as sample size increases. . #collapse-hide # The following function generates samples with difference # sizes from an exponential distribution. def MakeExpoSamples(beta=2.0, iters=1000): &quot;&quot;&quot;Generates samples from an exponential distribution. beta: parameter iters: number of samples to generate for each size returns: list of samples &quot;&quot;&quot; samples = [] for n in [1, 10, 100]: sample = [np.sum(np.random.exponential(beta, n)) for _ in range(iters)] samples.append((n, sample)) return samples # This function generates normal probability plots for # samples with various sizes. def NormalPlotSamples(samples, plot=1, ylabel=&#39;&#39;): &quot;&quot;&quot;Makes normal probability plots for samples. samples: list of samples label: string &quot;&quot;&quot; for n, sample in samples: thinkplot.SubPlot(plot) thinkstats2.NormalProbabilityPlot(sample) thinkplot.Config(title=&#39;n=%d&#39; % n, legend=False, xticks=[], yticks=[], xlabel=&#39;Random normal variate&#39;, ylabel=ylabel) plot += 1 # Make plots. thinkplot.PrePlot(num=3, rows=2, cols=3) samples = MakeExpoSamples() NormalPlotSamples(samples, plot=1, ylabel=&#39;Sum of expo values&#39;) . . . Lognormal Distribution . The lognormal distribution has higher variance, so it requires a larger sample size before it converges to normal. . #collapse-hide # The following function generates samples with difference # sizes from an lognormal distribution. def MakeLognormalSamples(mu=1.0, sigma=1.0, iters=1000): &quot;&quot;&quot;Generates samples from a lognormal distribution. mu: parmeter sigma: parameter iters: number of samples to generate for each size returns: list of samples &quot;&quot;&quot; samples = [] for n in [1, 10, 100]: sample = [np.sum(np.random.lognormal(mu, sigma, n)) for _ in range(iters)] samples.append((n, sample)) return samples # Make plots. thinkplot.PrePlot(num=3, rows=2, cols=3) samples = MakeLognormalSamples() NormalPlotSamples(samples, ylabel=&#39;sum of lognormal values&#39;) . . . Pareto Distribution . The Pareto distribution has infinite variance, and sometimes infinite mean, depending on the parameters. It violates the requirements of the CLT and does not generally converge to normal. . #collapse-hide # The following function generates samples with difference # sizes from an Pareto distribution. def MakeParetoSamples(alpha=1.0, iters=1000): &quot;&quot;&quot;Generates samples from a Pareto distribution. alpha: parameter iters: number of samples to generate for each size returns: list of samples &quot;&quot;&quot; samples = [] for n in [1, 10, 100]: sample = [np.sum(np.random.pareto(alpha, n)) for _ in range(iters)] samples.append((n, sample)) return samples # Make plots. thinkplot.PrePlot(num=3, rows=2, cols=3) samples = MakeParetoSamples() NormalPlotSamples(samples, ylabel=&#39;sum of Pareto values&#39;) . . . Distribution of Correlated Random Variates . If the random variates are correlated, that also violates the CLT, so the sums don&#39;t generally converge. To generate correlated values, we generate correlated normal values and then transform to whatever distribution we want. . #collapse-hide def GenerateCorrelated(rho, n): &quot;&quot;&quot;Generates a sequence of correlated values from a standard normal dist. rho: coefficient of correlation n: length of sequence returns: iterator &quot;&quot;&quot; x = random.gauss(0, 1) yield x sigma = np.sqrt(1 - rho**2) for _ in range(n-1): x = random.gauss(x * rho, sigma) yield x def GenerateExpoCorrelated(rho, n): &quot;&quot;&quot;Generates a sequence of correlated values from an exponential dist. rho: coefficient of correlation n: length of sequence returns: NumPy array &quot;&quot;&quot; normal = list(GenerateCorrelated(rho, n)) uniform = norm.cdf(normal) expo = expon.ppf(uniform) return expo def MakeCorrelatedSamples(rho=0.9, iters=1000): &quot;&quot;&quot;Generates samples from a correlated exponential distribution. rho: correlation iters: number of samples to generate for each size returns: list of samples &quot;&quot;&quot; samples = [] for n in [1, 10, 100]: sample = [np.sum(GenerateExpoCorrelated(rho, n)) for _ in range(iters)] samples.append((n, sample)) return samples # Make plots. thinkplot.PrePlot(num=3, rows=2, cols=3) samples = MakeCorrelatedSamples() NormalPlotSamples(samples, ylabel=&#39;Sum of correlated exponential values&#39;) . . .",
            "url": "https://nickmccarty.github.io/til/statistics/central%20limit%20theorem/python/2020/04/30/central-limit-theorem.html",
            "relUrl": "/statistics/central%20limit%20theorem/python/2020/04/30/central-limit-theorem.html",
            "date": " • Apr 30, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": ". This site is an attempt at organizing knowledge acquired in the pursuit of turning questions and problems into useful data products. .",
          "url": "https://nickmccarty.github.io/til/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nickmccarty.github.io/til/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}